{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import pyttsx3\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import speech_recognition as sr\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Administrator/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-6-2 Python-3.10.7 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "#Loading pre-trained YOLO model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Text-to-Speech Engine\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize OpenAI API(Use your Own People)\n",
    "openai.api_key = 'Key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objects_details(object_name):\n",
    "    try:\n",
    "        url = f\"htps://end.wikipedia.org/api/rest_v1/page/summary/{object_name}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('extract', f\"Information about {object_name}\")\n",
    "        else:\n",
    "            return f\"Information about {object_name} is not available.\"\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Could not retrieve information due to an error: {str(e)}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(bbox, frame_width):\n",
    "    center_x = (bbox[0] + bbox[2]) /2\n",
    "    if center_x < frame_width / 3:\n",
    "        return \"left\"\n",
    "    elif center_x > 2 *frame_width /3:\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"center\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    results = model(frame)\n",
    "    labels, cord = results.xyxyn[0][:, -1].numpy(), results.xyxyn[0][:, :-1].numpy()\n",
    "    frame_width = frame.shape[1]\n",
    "\n",
    "    detected_objects = []\n",
    "    for i, (label, bbox) in enumerate(zip(labels, cord)) :\n",
    "        class_name = model.names[int(label)]\n",
    "        x1,y1,x2,y2, conf = bbox\n",
    "        x1,y1,x2,y2 = int(x1 * frame_width), int(y1 * frame.shape[0]), int(x2 * frame_width), int(y2 * frame_width[0])\n",
    "\n",
    "        position = get_position([x1,y1,x2,y2],  frame_width)\n",
    "\n",
    "        #draw bounding box\n",
    "        cv2.rectangle(frame, (x1,y1) , (x2,y2), (255, 0,0), 2)\n",
    "        cv2.putText(frame, f\"{class_name} {conf: .2f}\", (x1,y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0))\n",
    "\n",
    "        speak(f\"I see a {class_name} on the {position}.\")\n",
    "\n",
    "        details = get_object_details(class_name)\n",
    "        speak(details)\n",
    "\n",
    "        detected_objects.append({\"class_name\": class_name, \"position\": position, \"details\": details})\n",
    "        time.sleep(2) #Pause to allow TTs to finish speaking\n",
    "    return frame, detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speech():\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "\n",
    "    with mic as source:\n",
    "        print (\"Listening....\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "    try:\n",
    "        command = recognizer.recognize_google(audio)\n",
    "        print(f\"Recognized: {command}\")\n",
    "        return command.lower()\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Sorry, I didn't catch that.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Could not request results; {e}\"\n",
    "from transformers import pipeline\n",
    "\n",
    "#Initialize the Hugging Face pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
