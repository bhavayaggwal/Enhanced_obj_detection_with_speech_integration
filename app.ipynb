{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import pyttsx3\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import speech_recognition as sr\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Administrator/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-6-2 Python-3.10.7 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "#Loading pre-trained YOLO model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Text-to-Speech Engine\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize OpenAI API(Use your Own People)\n",
    "openai.api_key = 'Key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objects_details(object_name):\n",
    "    try:\n",
    "        url = f\"htps://end.wikipedia.org/api/rest_v1/page/summary/{object_name}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('extract', f\"Information about {object_name}\")\n",
    "        else:\n",
    "            return f\"Information about {object_name} is not available.\"\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Could not retrieve information due to an error: {str(e)}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(bbox, frame_width):\n",
    "    center_x = (bbox[0] + bbox[2]) /2\n",
    "    if center_x < frame_width / 3:\n",
    "        return \"left\"\n",
    "    elif center_x > 2 *frame_width /3:\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"center\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    results = model(frame)\n",
    "    labels, cord = results.xyxyn[0][:, -1].numpy(), results.xyxyn[0][:, :-1].numpy()\n",
    "    frame_width = frame.shape[1]\n",
    "\n",
    "    detected_objects = []\n",
    "    for i, (label, bbox) in enumerate(zip(labels, cord)) :\n",
    "        class_name = model.names[int(label)]\n",
    "        x1,y1,x2,y2, conf = bbox\n",
    "        x1,y1,x2,y2 = int(x1 * frame_width), int(y1 * frame.shape[0]), int(x2 * frame_width), int(y2 * frame_width[0])\n",
    "\n",
    "        position = get_position([x1,y1,x2,y2],  frame_width)\n",
    "\n",
    "        #draw bounding box\n",
    "        cv2.rectangle(frame, (x1,y1) , (x2,y2), (255, 0,0), 2)\n",
    "        cv2.putText(frame, f\"{class_name} {conf: .2f}\", (x1,y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0))\n",
    "\n",
    "        speak(f\"I see a {class_name} on the {position}.\")\n",
    "\n",
    "        details = get_object_details(class_name)\n",
    "        speak(details)\n",
    "\n",
    "        detected_objects.append({\"class_name\": class_name, \"position\": position, \"details\": details})\n",
    "        time.sleep(2) #Pause to allow TTs to finish speaking\n",
    "    return frame, detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading config.json: 100%|██████████| 473/473 [00:00<?, ?B/s] \n",
      "Downloading model.safetensors: 100%|██████████| 261M/261M [01:08<00:00, 3.79MB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 49.0/49.0 [00:00<?, ?B/s]\n",
      "Downloading vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 528kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 661kB/s]\n"
     ]
    }
   ],
   "source": [
    "def recognize_speech():\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "\n",
    "    with mic as source:\n",
    "        print (\"Listening....\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "    try:\n",
    "        command = recognizer.recognize_google(audio)\n",
    "        print(f\"Recognized: {command}\")\n",
    "        return command.lower()\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Sorry, I didn't catch that.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Could not request results; {e}\"\n",
    "from transformers import pipeline\n",
    "\n",
    "#Initialize the Hugging Face pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "def ask_openai(question, context=\"\"):\n",
    "    try:\n",
    "        response = qa_pipeline(question=question, context=context)\n",
    "        answer = response['answer']\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f\"An error occured while getting the response: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_command(command, detected_objects):\n",
    "    context = \" \".join([obj['details'] for obj in detected_objects])\n",
    "    if \"repeat\" in command:\n",
    "        speak(\"I will repeat the information.\")\n",
    "        for obj in detected_objects:\n",
    "            speak(f\"I see a {obj['class_name']} on the {obj['position']}. {obj['details']}\")\n",
    "    elif \"details\" in command:\n",
    "        for obj in detected_objects:\n",
    "            if obj['class_name'] in command:\n",
    "                speak(f\"Here are more details about {obj['class_name']}: {obj['details']}\")\n",
    "    else:\n",
    "        answer = ask_openai(command, context)\n",
    "        speak(answer)\n",
    "        speak(\"Do you need any further assistance?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(source_type=\"camera\"):\n",
    "    source = 0 if source_type == \"camera\" else source_type\n",
    "    cap = cv2.VideoCapture(source)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video source {source}\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame, detected_objects = process_frame(frame)\n",
    "        cv2.imshow('YOLO Object Detection', frame)\n",
    "\n",
    "        if cv2.waitkey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        command = recognize_speech()\n",
    "        if command:\n",
    "            handle_command(command, detected_objects)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_type = input(\"Enter source type (camera/file): \").strip()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
